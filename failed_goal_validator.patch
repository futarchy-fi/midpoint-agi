diff --git a/src/midpoint/agents/goal_decomposer.py b/src/midpoint/agents/goal_decomposer.py
index 5ebc427..ce79581 100644
--- a/src/midpoint/agents/goal_decomposer.py
+++ b/src/midpoint/agents/goal_decomposer.py
@@ -109,7 +109,7 @@ from midpoint.agents.tools import (
     retrieve_memory_documents,
 )
 from midpoint.agents.config import get_openai_api_key
-from midpoint.utils.logging import log_manager
+# from midpoint.utils.logging import log_manager # Commented out - Circular Dep / Unused
 from dotenv import load_dotenv
 import subprocess
 import time
diff --git a/src/midpoint/agents/goal_validator.py b/src/midpoint/agents/goal_validator.py
index a589aee..3090b6b 100644
--- a/src/midpoint/agents/goal_validator.py
+++ b/src/midpoint/agents/goal_validator.py
@@ -6,6 +6,7 @@ task execution results. It MUST NOT contain any task-specific logic or hardcoded
 All validation decisions should be made by the LLM at runtime.
 """
 
+import asyncio
 import json
 import logging
 import re
@@ -14,11 +15,13 @@ from typing import List, Dict, Any, Optional, Tuple
 import os
 import random
 from pathlib import Path
+import subprocess
 
 from openai import AsyncOpenAI
 
 from midpoint.agents.models import Goal, ExecutionResult, CriterionResult, ValidationResult, State
 from midpoint.agents.tools.git_tools import get_current_hash, get_current_branch
+from midpoint.agents.tools.memory_tools import get_memory_diff
 from .tools import (
     list_directory,
     read_file,
@@ -27,6 +30,8 @@ from .tools import (
     web_search,
     web_scrape
 )
+# Import validate_repository_state from goal_decomposer
+from .goal_decomposer import validate_repository_state
 from .tools.processor import ToolProcessor
 from .tools.registry import ToolRegistry
 from .config import get_openai_api_key
@@ -34,42 +39,33 @@ from .config import get_openai_api_key
 # Set up logging
 logger = logging.getLogger(__name__)
 
-# System prompt for the validator - MODIFIED
+# System prompt for the validator
 VALIDATION_SYSTEM_PROMPT = """
 You are the Goal Validator, an expert at verifying whether a goal's validation criteria have been met.
 
-Your task is to determine if the work done for a goal meets its validation criteria.
-Follow an OODA loop:
+Your task is to analyze the evidence provided and determine if each validation criterion has been met.
+You should look for concrete evidence in the repository changes and any other information provided.
 
-1.  **OBSERVE:** You will be given the initial and potentially final state hashes (git and memory). Use available tools (`run_terminal_cmd`, `read_file`, potentially others) to:
-    *   Verify you are on the correct git branch specified in the execution result.
-    *   Get the repository diff (`git diff <initial_hash> <final_hash> | cat`).
-    *   Get the memory diff (e.g., using `git diff` in the memory repo if applicable, or by reading relevant files if needed).
-    *   Read specific files if needed to understand the changes related to the criteria.
+Be precise and objective in your assessment. Clearly explain your reasoning for each criterion.
+You should provide specific evidence from the diffs or other sources to support your conclusions.
 
-2.  **ORIENT:** Analyze the evidence gathered (diffs, file contents) in relation to the goal's validation criteria.
-
-3.  **DECIDE:** For each validation criterion, determine if the evidence shows it has been met. Be precise and objective.
-
-4.  **OUTPUT:** Provide ONLY a valid JSON object with this exact structure:
+YOUR RESPONSE MUST BE A VALID JSON OBJECT with this exact structure:
     {
         "criteria_results": [
             {
                 "criterion": "string",
                 "passed": boolean,
                 "reasoning": "string",
-                "evidence": ["string"] // Specific references to diffs or file contents
-            }
-        ],
-        "score": float, // Overall score 0.0-1.0 based on passed criteria
-        "reasoning": "string" // Overall reasoning for the score
-    }
-
-IMPORTANT:
-- Base your validation ONLY on the changes between the initial and final states provided.
-- Cite specific evidence from the diffs or files you examined.
-- Do not include any explanatory text before or after the JSON object.
-- If you cannot gather necessary evidence with tools (e.g., tool error, insufficient info), reflect this failure in the reasoning and mark relevant criteria as failed.
+            "evidence": ["string"]
+        }
+    ],
+    "score": float,
+    "reasoning": "string"
+}
+
+Do not include any explanatory text before or after the JSON object.
+Just output the JSON structure directly.
+If you cannot properly validate, still return a valid JSON object with appropriate failure messages.
 """
 
 class GoalValidator:
@@ -80,7 +76,7 @@ class GoalValidator:
     - Remain completely task-agnostic
     - Not contain any hardcoded validation rules
     - Delegate all validation decisions to the LLM
-    - Let the LLM use tools to gather evidence for validation
+    - Use the provided tools to gather evidence for validation
 
     As outlined in the VISION.md document, this validator:
     1. Evaluates whether a subgoal has been successfully achieved
@@ -111,17 +107,57 @@ class GoalValidator:
         self.last_validation_messages = None
         
         # System prompt for the LLM that will make all validation decisions
-        self.system_prompt = VALIDATION_SYSTEM_PROMPT # Use updated prompt
+        self.system_prompt = """You are a goal validation agent responsible for evaluating execution results.
+Your role is to:
+1. Check if execution was successful
+2. Validate changes against goal criteria
+3. Provide detailed reasoning for validation decisions
+4. Calculate a validation score
+
+IMPORTANT: You must be thorough and objective in your validation. Your job is to ensure that each validation criterion 
+has been properly satisfied. Be specific in your reasoning and cite concrete evidence rather than making general statements.
+
+For each validation criterion:
+1. Use the available tools to gather evidence about the changes
+2. Analyze the repository and memory diffs to find relevant changes
+3. Examine how the changes relate to the specific validation criterion
+4. Determine if the changes satisfy the criterion
+5. Provide clear reasoning with specific references to parts of the diffs
+
+When analyzing diffs:
+- Look for file additions, modifications, and deletions
+- Check file content changes and their relationship to the criteria
+- Examine memory documents that were added or modified
+- Consider both the quantity and quality of changes
+
+Focus on the SPECIFIC CHANGES that occurred from initial to final state, not just the final state in isolation.
+Your validation must explicitly reference evidence from the diffs when available.
+
+Your response must be in JSON format with these fields:
+{
+    "criteria_results": [
+        {
+            "criterion": "string",
+            "passed": boolean,
+            "reasoning": "string",
+            "evidence": ["string"]  // Specific references to parts of the diffs
+        }
+    ],
+    "overall_score": float,  // Between 0 and 1
+    "overall_reasoning": "string"
+}"""
 
         # Get tools from registry
         self.tools = ToolRegistry.get_tool_schemas()
 
+    # This method should be synchronous as it's called from synchronous contexts
     def validate_execution(self, goal: Goal, execution_result: ExecutionResult) -> ValidationResult:
         """
         Validate execution results against a goal using LLM.
         
-        NOTE: This synchronous method relies on its caller providing an asyncio event loop
-        because it internally calls async helper functions/tools.
+        This method takes a goal and execution result and validates the execution
+        results against the goal using an LLM. This method handles repository
+        state validation and content validation.
         
         Args:
             goal: The goal to validate
@@ -140,96 +176,245 @@ class GoalValidator:
                 validated_by="System",
                 automated=True,
                 repository_state=None,
-                reasoning="Execution reported as failed."
             )
         
-        # Get current state information to provide context to the LLM
-        current_repo_path = execution_result.repository_path
-        current_branch = "unknown"
-        current_hash = "unknown"
+        # Validate repository state
+        repo_info = {}
         try:
-            # Call async helper directly, relies on caller's loop
-            current_branch = get_current_branch(current_repo_path)
-            # Call async helper directly, relies on caller's loop
-            current_hash = get_current_hash(current_repo_path)
-            logging.info(f"Current state for validation: Branch='{current_branch}', Hash='{current_hash[:8]}'")
+            # Use synchronous call from utils
+            current_branch = get_current_branch(execution_result.repository_path)
+            if current_branch != execution_result.branch_name:
+                logging.info(f"Current branch {current_branch} does not match execution branch {execution_result.branch_name}")
+                # Try to switch to correct branch using subprocess
+                try:
+                    proc = subprocess.run(
+                        ["git", "checkout", execution_result.branch_name],
+                        cwd=execution_result.repository_path,
+                        capture_output=True,
+                        text=True
+                    )
+                    if proc.returncode == 0:
+                        logging.info(f"Switched to branch {execution_result.branch_name}")
+                    else:
+                        raise ValueError(f"Failed to checkout branch: {proc.stderr}")
+                except Exception as e:
+                    logging.error(f"Failed to switch to branch {execution_result.branch_name}: {e}")
+                    # Return failed validation
+                    return ValidationResult(
+                        goal_id=goal.id,
+                        timestamp=datetime.now().strftime("%Y%m%d_%H%M%S"),
+                        criteria_results=[],
+                        score=0.0,
+                        validated_by="System",
+                        automated=True,
+                        repository_state=State(git_hash=current_hash, branch_name=current_branch, repository_path=execution_result.repository_path, description="Repository state at time of validation failure") if current_hash != "unknown" else None,
+                        reasoning=f"Automated validation failed with exception: {str(e)}"
+                    )
+        
+            # Get repository info
+            # Use synchronous call from utils
+            current_hash = get_current_hash(execution_result.repository_path)
+            repo_info = {
+                "git_hash": current_hash,
+                "branch_name": current_branch,
+                "repository_path": execution_result.repository_path,
+                "description": "Goal validation state"
+            }
         except Exception as e:
-            logging.error(f"Failed to get initial repository state: {e}")
-            # Proceed, but LLM will need to verify/handle this
-
-        # Extract initial state info from the goal object
-        initial_git_hash = None
-        initial_memory_hash = None
-        if hasattr(goal, 'initial_state') and goal.initial_state:
-            initial_git_hash = getattr(goal.initial_state, 'git_hash', None)
-            initial_memory_hash = getattr(goal.initial_state, 'memory_hash', None)
-
-        # Extract current memory hash from the goal object (if available)
-        current_memory_hash = None
-        if hasattr(goal, 'current_state') and goal.current_state:
-             current_memory_hash = getattr(goal.current_state, 'memory_hash', None)
-        # Fallback to execution result if not on goal
-        if not current_memory_hash and execution_result.final_state:
-             current_memory_hash = getattr(execution_result.final_state, 'memory_hash', None)
-
-        # Prepare the user prompt for the LLM
-        prompt_lines = [
-            f"Please validate the goal: '{goal.description}'",
-            "Based on the changes between the initial and current states.",
-            "\n**Goal Details:**",
-            f"- Description: {goal.description}",
-            f"- Validation Criteria:",
-        ]
-        prompt_lines.extend([f"  - {c}" for c in goal.validation_criteria])
-
-        prompt_lines.append("\n**State Information:**")
-        prompt_lines.append(f"- Target Branch: {execution_result.branch_name}") # Branch LLM should be on
-        prompt_lines.append(f"- Initial Git Hash: {initial_git_hash or 'Not specified'}")
-        prompt_lines.append(f"- Final Git Hash: {current_hash}") # The hash LLM should currently see
-        prompt_lines.append(f"- Initial Memory Hash: {initial_memory_hash or 'Not specified'}")
-        prompt_lines.append(f"- Final Memory Hash: {current_memory_hash or 'Not specified'}")
-        prompt_lines.append(f"- Repository Path: {current_repo_path}")
-        # We might need to provide memory repo path if it's separate and needed for tools
-        # memory_repo_path = ... # How to get this reliably? Assume tools can infer for now or add if needed.
-
-        prompt_lines.append("\n**Instructions:**")
-        prompt_lines.append("1. Verify you are on the target git branch using tools.")
-        prompt_lines.append("2. Use tools to get the repository diff between the initial and final git hashes.")
-        prompt_lines.append("3. Use tools to investigate memory changes between the initial and final memory hashes (e.g., diff or reading files)." if initial_memory_hash and current_memory_hash else "3. No memory comparison needed or possible.")
-        prompt_lines.append("4. Analyze the gathered evidence against the validation criteria.")
-        prompt_lines.append("5. Provide your assessment in the required JSON format.")
-
-        user_prompt = "\n".join(prompt_lines)
-
+            logging.error(f"Failed to get repository info: {e}")
+        
+        # Initialize placeholders for prompt content
+        additional_evidence_section = "" # Initialize to empty string
+        repo_diff = "(Diff content is provided via file path)"
+        if goal.metadata and goal.metadata.get("code_diff_error"):
+            repo_diff = f"Error retrieving code diff: {goal.metadata.get('code_diff_error')}"
+        elif goal.metadata and not goal.metadata.get("code_diff_path"):
+            repo_diff = "(No code diff applicable or generated)"
+        
+        memory_diff_content = "No memory changes detected"
+        memory_diff_files = []
+        try:
+            if (hasattr(goal, 'initial_state') and goal.initial_state and 
+                hasattr(goal.initial_state, 'memory_hash') and goal.initial_state.memory_hash and
+                hasattr(goal, 'current_state') and goal.current_state and
+                hasattr(goal.current_state, 'memory_hash') and goal.current_state.memory_hash):
+                # get_memory_diff is synchronous
+                memory_diff = get_memory_diff(goal.initial_state.memory_hash, 
+                                            goal.current_state.memory_hash,
+                                            execution_result.repository_path)
+                
+                if memory_diff:
+                    memory_diff_content = memory_diff.get('diff_content', 'No memory changes detected')
+                    memory_diff_files = memory_diff.get('changed_files', [])
+                    
+                    # If memory hashes are different but no changes detected, something's wrong
+                    # Add details about the changed files
+                    if (memory_diff_content == "No memory changes detected" or not memory_diff_content.strip()) and \
+                       goal.initial_state.memory_hash != goal.current_state.memory_hash:
+                        logging.warning(f"Memory hashes differ ({goal.initial_state.memory_hash} vs {goal.current_state.memory_hash}) but no diff content found")
+                        
+                        # Get the content of any changed files
+                        if memory_diff_files:
+                            memory_repo_path = memory_diff.get('memory_repo_path', execution_result.repository_path)
+                            additional_content = ["Memory hashes differ but standard diff shows no changes. Showing file contents instead:"]
+                            
+                            for file_path in memory_diff_files:
+                                try:
+                                    # Save current state
+                                    current_branch = get_current_branch(memory_repo_path)
+                                    
+                                    # Try to checkout the final hash to read files
+                                    checkout_proc = subprocess.run(
+                                        ["git", "checkout", goal.current_state.memory_hash],
+                                        cwd=memory_repo_path,
+                                        capture_output=True,
+                                        text=True
+                                    )
+                                    
+                                    if checkout_proc.returncode == 0:
+                                        # Try to read the file content
+                                        file_full_path = os.path.join(memory_repo_path, file_path)
+                                        if os.path.exists(file_full_path):
+                                            with open(file_full_path, 'r') as f:
+                                                file_content = f.read()
+                                            additional_content.append(f"\n--- FILE: {file_path} ---\n{file_content}")
+                                    
+                                    # Restore original branch
+                                    subprocess.run(
+                                        ["git", "checkout", current_branch],
+                                        cwd=memory_repo_path,
+                                        capture_output=True,
+                                        text=True
+                                    )
+                                except Exception as file_error:
+                                    logging.error(f"Error reading changed file {file_path}: {file_error}")
+                                    additional_content.append(f"Error reading {file_path}: {str(file_error)}")
+                        
+                            if len(additional_content) > 1:  # If we have any file content
+                                memory_diff_content = "\n".join(additional_content)
+            
+                    # Auto-fail if memory diff is too large (over 100KB)
+                    if len(memory_diff_content) > 100000:
+                        logging.warning(f"Memory diff is too large ({len(memory_diff_content)} bytes). Auto-failing validation.")
+                        failed_criteria = []
+                        for criterion in goal.validation_criteria:
+                            failed_criteria.append(
+                                CriterionResult(
+                                    criterion=criterion,
+                                    passed=False,
+                                    reasoning="Validation failed due to memory changes being too large to validate",
+                                    evidence=["Memory diff exceeded maximum size (100KB)"]
+                                )
+                            )
+                        return ValidationResult(
+                            goal_id=goal.id,
+                            timestamp=datetime.now().strftime("%Y%m%d_%H%M%S"),
+                            criteria_results=failed_criteria,
+                            score=0.0,
+                            validated_by="System",
+                            automated=True,
+                            repository_state=State(**repo_info) if repo_info else None,
+                            reasoning="Memory changes were too large to validate"
+                        )
+        except Exception as e:
+            logging.error(f"Failed to get memory diff: {e}")
+            memory_diff_content = f"Error getting memory diff: {str(e)}"
+        
+        # If memory hashes are different but no diff content, fail validation immediately
+        if (hasattr(goal, 'initial_state') and goal.initial_state and 
+            hasattr(goal.initial_state, 'memory_hash') and goal.initial_state.memory_hash and
+            hasattr(goal, 'current_state') and goal.current_state and
+            hasattr(goal.current_state, 'memory_hash') and goal.current_state.memory_hash and
+            goal.initial_state.memory_hash != goal.current_state.memory_hash and
+            (memory_diff_content == "No memory changes detected" or memory_diff_content.startswith("Error getting memory diff"))):
+            
+            logging.error(f"Memory hash changed from {goal.initial_state.memory_hash} to {goal.current_state.memory_hash} but diffs could not be obtained correctly")
+            
+            # Auto-fail validation
+            failed_criteria = []
+            for criterion in goal.validation_criteria:
+                failed_criteria.append(
+                    CriterionResult(
+                        criterion=criterion,
+                        passed=False,
+                        reasoning="Validation failed because memory diffs could not be obtained correctly",
+                        evidence=[f"Memory hash changed from {goal.initial_state.memory_hash} to {goal.current_state.memory_hash} but diff tools failed to show changes"]
+                    )
+                )
+            return ValidationResult(
+                goal_id=goal.id,
+                timestamp=datetime.now().strftime("%Y%m%d_%H%M%S"),
+                criteria_results=failed_criteria,
+                score=0.0,
+                validated_by="System",
+                automated=True,
+                repository_state=State(**repo_info) if repo_info else None,
+                reasoning="Memory diffs could not be obtained correctly"
+            )
+        
+        # Prepare context for LLM
         messages = [
-            {"role": "system", "content": self.system_prompt},
-            {"role": "user", "content": user_prompt}
-        ]
+            {"role": "system", "content": VALIDATION_SYSTEM_PROMPT},
+            {"role": "user", "content": f"""
+You are tasked with validating whether the execution results match the goal criteria.
+
+### Goal
+{goal.description}
 
-        # Store the messages for later saving (consider if this is still needed/useful)
+### Validation Criteria
+{json.dumps(goal.validation_criteria, indent=2)}
+
+### Evidence
+
+#### Repository Changes
+```diff
+{repo_diff}
+```
+
+#### Memory Repository Changes
+```diff
+{memory_diff_content}
+```
+
+#### Memory Changed Files
+{json.dumps(memory_diff_files, indent=2) if memory_diff_files else "No files changed in memory"}
+
+{additional_evidence_section}
+
+Evaluate the evidence above to determine if the goal's validation criteria are met. 
+Do not perform any new actions or tool calls to satisfy the criteria.
+Only validate based on the evidence provided from previous execution.
+"""
+        },
+    ]
+        
+        # Store the messages for later saving
         self.last_validation_messages = messages
 
         try:
-            # Use the LLM to perform validation, allowing tool use
-            validation_response_content = self._validate_with_tools(goal, messages)
+            # Use the LLM to validate the goal
+            # _validate_with_tools is synchronous
+            validation_response = self._validate_with_tools(goal, messages)
 
             # Log the raw response for debugging
-            logging.info(f"Raw validation response (first 200 chars): {validation_response_content[:200] if validation_response_content else 'empty'}")
+            logging.info(f"Raw validation response (first 200 chars): {validation_response[:200] if validation_response else 'empty'}")
 
-            # Extract criteria_results from validation_response
-            criteria_data = self._extract_validation_json(validation_response_content)
+            # Extract criteria_results from validation_response and convert to ValidationResult
+            criteria_data = self._extract_validation_json(validation_response)
 
             if not criteria_data or "criteria_results" not in criteria_data:
-                logging.error("No valid validation criteria results found in LLM response")
-                # Create a generic failure if JSON is invalid
-                failed_criteria = [
+                logging.error("No validation criteria results found in response")
+                # Return failed validation with error message
+                failed_criteria = []
+                for criterion in goal.validation_criteria:
+                    failed_criteria.append(
                     CriterionResult(
-                        criterion=c,
+                            criterion=criterion,
                         passed=False,
-                        reasoning="Validation failed: Could not parse valid JSON from LLM response.",
-                        evidence=[f"Raw response: {validation_response_content[:200]}..."]
-                    ) for c in goal.validation_criteria
-                ]
+                            reasoning=f"Validation failed due to error: Could not extract valid JSON from LLM response",
+                            evidence=[]
+                        )
+                    )
                 return ValidationResult(
                     goal_id=goal.id,
                     timestamp=datetime.now().strftime("%Y%m%d_%H%M%S"),
@@ -237,56 +422,32 @@ class GoalValidator:
                     score=0.0,
                     validated_by="LLM",
                     automated=True,
-                    repository_state=State(git_hash=current_hash, branch_name=current_branch, repository_path=current_repo_path) if current_hash != "unknown" else None,
-                    reasoning="Could not extract valid JSON from LLM response"
+                    repository_state=State(**repo_info) if repo_info else None,
                 )
 
             # Create criterion results from validation data
             criteria_results = []
-            all_criteria_found = True
-            goal_criteria_set = set(goal.validation_criteria)
-            response_criteria_set = set()
             for criteria in criteria_data.get("criteria_results", []):
-                criterion_text = criteria.get("criterion", "")
-                response_criteria_set.add(criterion_text)
                 criteria_results.append(
                     CriterionResult(
-                        criterion=criterion_text,
+                        criterion=criteria.get("criterion", ""),
                         passed=criteria.get("passed", False),
                         reasoning=criteria.get("reasoning", ""),
                         evidence=criteria.get("evidence", [])
-                    )
-                )
-            # Check if LLM missed any criteria
-            missing_criteria = goal_criteria_set - response_criteria_set
-            if missing_criteria:
-                all_criteria_found = False
-                for criterion in missing_criteria:
-                    criteria_results.append(
-                        CriterionResult(
-                            criterion=criterion,
-                            passed=False,
-                            reasoning="Validation failed: LLM did not provide an assessment for this criterion.",
-                            evidence=[]
                         )
                     )
 
             # Calculate score
-            score = criteria_data.get("score", criteria_data.get("overall_score")) # Try both keys
-            reasoning = criteria_data.get("reasoning", criteria_data.get("overall_reasoning", ""))
+            score = criteria_data.get("score", 0.0)
+            if score > 1.0:  # Normalize score if it's given as percentage
+                score = score / 100.0
 
-            # If score is missing or invalid, recalculate based on results
-            if score is None or not isinstance(score, (float, int)):
-                logging.warning("Score missing or invalid in LLM response, recalculating.")
+            if score == 0.0 and criteria_results:
+                # Calculate score based on passed criteria if not provided
                 passed_count = sum(1 for cr in criteria_results if cr.passed)
                 total_count = len(criteria_results)
-                score = (passed_count / total_count) if total_count > 0 else 0.0
-            elif score > 1.0: # Normalize score if it's given as percentage
-                score = score / 100.0
-
-            # Add note about missing criteria to reasoning if needed
-            if not all_criteria_found:
-                reasoning += f"\nNote: LLM did not evaluate all criteria ({len(missing_criteria)} missing)."
+                if total_count > 0:
+                    score = passed_count / total_count
 
             # Return validation result
             return ValidationResult(
@@ -296,13 +457,11 @@ class GoalValidator:
                 score=score,
                 validated_by="LLM",
                 automated=True,
-                # Provide the state as observed *before* LLM interaction
-                repository_state=State(git_hash=current_hash, branch_name=current_branch, repository_path=current_repo_path) if current_hash != "unknown" else None,
+                repository_state=State(git_hash=current_hash, branch_name=current_branch, repository_path=execution_result.repository_path, description="Repository state at time of validation") if current_hash != "unknown" else None,
                 reasoning=reasoning
             )
-
         except Exception as e:
-            logging.error(f"Error during automated validation: {e}", exc_info=True) # Add traceback
+            logging.error(f"Error during automated validation: {e}")
             # Create failed criteria results for each validation criterion
             failed_criteria = []
             for criterion in goal.validation_criteria:
@@ -310,7 +469,7 @@ class GoalValidator:
                     CriterionResult(
                         criterion=criterion,
                         passed=False,
-                        reasoning=f"Validation failed due to system error: {str(e)}",
+                        reasoning=f"Validation failed due to error: {str(e)}",
                         evidence=[]
                     )
                 )
@@ -321,10 +480,10 @@ class GoalValidator:
                 timestamp=datetime.now().strftime("%Y%m%d_%H%M%S"),
                 criteria_results=failed_criteria,
                 score=0.0,
-                validated_by="System", # System error, not LLM error
+                validated_by="LLM",
                 automated=True,
-                repository_state=State(git_hash=current_hash, branch_name=current_branch, repository_path=current_repo_path) if current_hash != "unknown" else None,
-                reasoning=f"Automated validation failed with exception: {str(e)}"
+                repository_state=State(git_hash=current_hash, branch_name=current_branch, repository_path=execution_result.repository_path, description="Repository state at time of validation") if current_hash != "unknown" else None,
+                reasoning=reasoning
             )
     
     def _generate_criterion_reasoning(self, criterion: str, passed: bool, evidence: List[str]) -> str:
@@ -370,13 +529,10 @@ class GoalValidator:
         
         return "\n".join(reasoning)
 
-    def validate_goal(self, goal_path: str, repository_path: str = ".") -> ValidationResult:
+    async def validate_goal(self, goal_path: str, repository_path: str = ".") -> ValidationResult:
         """
         Validate a goal using a dummy execution result.
         
-        NOTE: This synchronous method relies on its caller providing an asyncio event loop
-        for the internal calls to async helpers and self.validate_execution.
-        
         Args:
             goal_path: Path to the goal JSON file
             repository_path: Path to the repository
@@ -397,80 +553,48 @@ class GoalValidator:
             initial_memory_hash = initial_state.get('memory_hash')
             initial_timestamp = initial_state.get('timestamp')
             
-            # Use current_state hash from goal file as the definitive 'final' hash
             current_git_hash = current_state.get('git_hash')
             current_memory_hash = current_state.get('memory_hash')
             current_timestamp = current_state.get('timestamp')
 
-            if not current_git_hash:
-                logging.warning(f"Current git hash not found in goal file {goal_path}, attempting to get from repo.")
-                current_git_hash = get_current_hash(repository_path)
-            if not current_git_hash:
-                 raise ValueError(f"Could not determine current git hash for validation from {goal_path} or repository.")
-
-            logging.debug(f"Using Initial Git Hash: {initial_git_hash}")
-            logging.debug(f"Using Final Git Hash:   {current_git_hash}")
-            logging.debug(f"Using Initial Mem Hash: {initial_memory_hash}")
-            logging.debug(f"Using Final Mem Hash:   {current_memory_hash}")
-
-            # Get the branch name associated with the current state hash
-            # This might require a separate lookup or be stored in the goal file?
-            # For now, fetch from the repo, assuming we're on the right branch.
-            current_branch_name = get_current_branch(repository_path)
-            logging.debug(f"Assuming current branch is: {current_branch_name}")
+            # Log hashes for debugging
+            logging.debug(f"Initial hash: {initial_git_hash}")
+            logging.debug(f"Current hash: {current_git_hash}")
 
             # Create Goal object
             goal = Goal(
                 id=goal_data.get('goal_id', ''),
                 description=goal_data.get('description', ''),
                 validation_criteria=goal_data.get('validation_criteria', []),
-                success_threshold=goal_data.get("success_threshold", 80.0),
+                success_threshold=80.0,  # Default threshold
                 initial_state=State(
                     git_hash=initial_git_hash,
                     memory_hash=initial_memory_hash,
                     timestamp=initial_timestamp
                 ),
-                current_state=State( # Pass current state from goal file
+                current_state=State(
                     git_hash=current_git_hash,
                     memory_hash=current_memory_hash,
                     timestamp=current_timestamp
                 )
             )
             
-            # Create ExecutionResult reflecting the state defined in the goal file
+            # Create dummy execution result
             execution_result = ExecutionResult(
-                success=True, # Assume success to trigger validation
+                success=True,
                 repository_path=repository_path,
-                branch_name=current_branch_name, # Branch associated with final state
-                git_hash=current_git_hash,    # Final hash
+                branch_name=get_current_branch(repository_path),
+                git_hash=current_git_hash,
                 task_id='',
                 goal_id=goal_data.get('goal_id', ''),
-                error_message=None,
-                # Include final state based on goal file's current_state
-                final_state=State(
-                    git_hash=current_git_hash,
-                    memory_hash=current_memory_hash,
-                    timestamp=current_timestamp,
-                    repository_path=repository_path,
-                    branch_name=current_branch_name
-                )
+                error_message=None
             )
             
-            # Validate execution (sync call)
-            return self.validate_execution(goal, execution_result)
+            # Validate execution
+            return await self.validate_execution(goal, execution_result)
         except Exception as e:
-            logging.error(f"Error validating goal: {e}", exc_info=True)
-            # Return a structured error
-            return ValidationResult(
-                 goal_id=goal_path, # Use path as fallback ID
-                 timestamp=datetime.now().strftime("%Y%m%d_%H%M%S"),
-                 criteria_results=[],
-                 score=0.0,
-                 validated_by="System",
-                 automated=True,
-                 repository_state=None,
-                 reasoning=f"Failed to set up validation for {goal_path}: {str(e)}"
-             )
+            logging.error(f"Error validating goal: {e}")
+            raise
 
     def _extract_validation_json(self, content: str) -> dict:
         """
@@ -684,14 +808,11 @@ class GoalValidator:
         logging.warning("No valid JSON validation response could be processed")
         return {"criteria_results": []} 
 
-    # Keep this method synchronous
+    # This method should be synchronous
     def _validate_with_tools(self, goal: Goal, messages: List[Dict[str, str]]) -> str:
         """
         Use LLM with tools to validate a goal.
         
-        NOTE: This synchronous method relies on its caller providing an asyncio event loop
-        for the internal call to self.tool_processor.run_llm_with_tools.
-        
         Args:
             goal: The goal to validate
             messages: The messages to send to the LLM, including system prompt and user query
@@ -699,10 +820,9 @@ class GoalValidator:
         Returns:
             The response content from the LLM
         """
-        logging.info(f"Validating goal {goal.id} with LLM using tools")
+        logging.info(f"Validating goal {goal.id} with LLM")
         
         # Run LLM with tools
-        # Call async tool processor directly, relies on caller's loop
         response, tool_calls = self.tool_processor.run_llm_with_tools(
             messages=messages,
             model=self.model
@@ -714,19 +834,46 @@ class GoalValidator:
         # Extract response from LLM
         if response and len(response) > 0:
             assistant_message = next((msg for msg in response if msg["role"] == "assistant"), None)
-            # The final message might just be the JSON, or could be preceded by tool calls
-            # We return the content of the *last* assistant message assuming it holds the final JSON
             if assistant_message and assistant_message.get("content"):
                 return assistant_message.get("content", "")
-            else:
-                # If the last message has no content (e.g., only tool calls), return empty
-                logging.warning("Final assistant message had no content.")
-                return ""
         
-        # If we get here, something went wrong, no response list or empty
-        logging.warning("No response messages received from LLM interaction.")
+        # If we didn't get a content response but got tool calls instead, report an error
+        if tool_calls:
+            logging.warning("Validator used tools instead of validating based on provided evidence")
+            
+            # Create a description of tools used, for debugging purposes
+            tool_descriptions = []
+            for i, tool_call in enumerate(tool_calls):
+                if tool_call.get("tool") and tool_call.get("args"):
+                    tool_name = tool_call.get("tool")
+                    args = tool_call.get("args", {})
+                    tool_descriptions.append(f"Used tool: {tool_name} with arguments: {json.dumps(args)}")
+            
+            # Return a validation response indicating tool usage isn't valid for validation
+            return json.dumps({
+                "criteria_results": [
+                    {
+                        "criterion": "Validation based on evidence",
+                        "passed": False,
+                        "reasoning": "The validator attempted to use tools instead of validating based on the provided evidence",
+                        "evidence": tool_descriptions
+                    }
+                ],
+                "score": 0.0,
+                "reasoning": "Validation should examine the provided evidence, not execute tools"
+            })
+        
+        # If we get here, we have neither content nor tool calls to work with
+        logging.warning("No content or tool calls in LLM response")
         return json.dumps({
-            "criteria_results": [],
+            "criteria_results": [
+                {
+                    "criterion": "Valid response required",
+                    "passed": False,
+                    "reasoning": "No content or tool calls in LLM response",
+                    "evidence": ["LLM did not provide a meaningful response"]
+                }
+            ],
             "score": 0.0,
-            "reasoning": "Validation failed: No response received from LLM."
+            "reasoning": "Validation failed due to empty LLM response"
         }) 
\ No newline at end of file
diff --git a/src/midpoint/goal_cli.py b/src/midpoint/goal_cli.py
index a81cfac..dd40952 100644
--- a/src/midpoint/goal_cli.py
+++ b/src/midpoint/goal_cli.py
@@ -21,11 +21,11 @@ import shutil
 from .agents.models import Goal, SubgoalPlan, TaskContext, ExecutionResult, MemoryState, State
 from .agents.goal_decomposer import decompose_goal as agent_decompose_goal
 from .agents.task_executor import TaskExecutor, configure_logging as configure_executor_logging
-from .agents.tools.git_tools import get_current_hash, get_current_branch, get_repository_diff
+from .agents.tools.git_tools import get_repository_diff
 from .agents.tools.memory_tools import get_memory_diff
 
 # Import validator for automated validation
-from .agents.goal_validator import GoalValidator
+from .validation import validate_goal # Import validate_goal directly
 
 # Import the new Goal Analyzer agent function
 from .agents.goal_analyzer import analyze_goal as agent_analyze_goal
@@ -33,6 +33,9 @@ from .agents.goal_analyzer import analyze_goal as agent_analyze_goal
 # Import the new parent update functions
 from .goal_operations.goal_update import propagate_success_state_to_parent, propagate_failure_history_to_parent
 
+# Import shared utilities
+from .utils import ensure_goal_dir, get_current_hash, get_current_branch
+
 # Configure basic logging
 logging.basicConfig(
     level=logging.INFO,
@@ -362,48 +365,6 @@ def create_new_task(parent_id, description):
     return task_id
 
 
-def get_current_hash(repo_path: Optional[str] = None) -> str:
-    """Get the current git hash.
-    
-    Args:
-        repo_path: Optional path to the git repository. If not provided, uses current directory.
-        
-    Returns:
-        The current git hash as a string.
-    """
-    try:
-        # If no repo_path provided, use current directory
-        if not repo_path:
-            repo_path = os.getcwd()
-            
-        result = subprocess.run(
-            ["git", "rev-parse", "HEAD"],
-            cwd=repo_path,
-            check=True,
-            capture_output=True,
-            text=True
-        )
-        return result.stdout.strip()
-    except subprocess.CalledProcessError as e:
-        logging.error(f"Failed to get current git hash: {e}")
-        return None
-
-
-def get_current_branch():
-    """Get the current git branch."""
-    try:
-        result = subprocess.run(
-            ["git", "rev-parse", "--abbrev-ref", "HEAD"],
-            check=True,
-            capture_output=True,
-            text=True
-        )
-        return result.stdout.strip()
-    except subprocess.CalledProcessError as e:
-        logging.error(f"Failed to get current branch: {e}")
-        return None
-
-
 def get_goal_id_from_branch(branch_name):
     """Extract goal ID from branch name."""
     # Branch naming convention: goal-G1-abcdef, goal-S1-abcdef, or goal-T1-abcdef
@@ -2023,6 +1984,70 @@ def main_command(args):
     elif args.command == "solve":
         return handle_solve_command(args)
     
+    # --- Add explicit call for validate command --- 
+    elif args.command == "validate":
+        print(f"--- Attempting validation for goal: {args.goal_id} ---")
+        try:
+            # Call the async validation function
+            # We need to wrap the async call
+            success, result = asyncio.run(validate_goal(
+                args.goal_id,
+                debug=args.debug,
+                quiet=args.quiet,
+                auto=args.auto
+            ))
+            print(f"--- Validation function returned: success={success} ---")
+
+            if success:
+                # If not auto, result contains goal_data and repo_state for manual validation
+                if not args.auto:
+                     # Handle manual validation (e.g., print criteria and wait for user)
+                     print(f"Manual validation needed for goal {args.goal_id}")
+                     # Placeholder: Implement manual validation UI here
+                else:
+                     # Handle auto validation result
+                     print(f"\nAutomated Validation Results for Goal {args.goal_id}:")
+                     # Check if result dictionary is populated
+                     if not result:
+                          print("ERROR: Validation reported success but returned empty result dictionary.")
+                          return False
+                         
+                     score = result.get("score", 0.0)
+                     criteria_results = result.get("criteria_results", [])
+                     passed_count = sum(1 for res in criteria_results if res.get("passed"))
+                     total_count = len(criteria_results)
+
+                     print(f"  Overall Score: {score:.2%} ({passed_count}/{total_count} criteria passed)")
+                     print(f"  Validated By: {result.get('validated_by', 'Unknown')}")
+                     print(f"  Timestamp: {result.get('timestamp', 'N/A')}")
+                     print(f"  Git Hash: {result.get('git_hash', 'N/A')[:8]}")
+
+                     # Optionally print detailed criteria results
+                     if not args.quiet:
+                         print("\n  Criteria Details:")
+                         if not criteria_results:
+                              print("    No criteria results found.")
+                         else:
+                              for i, res in enumerate(criteria_results, 1):
+                                  criterion = res.get("criterion", "Unknown Criterion")
+                                  passed = res.get("passed", False)
+                                  reasoning = res.get("reasoning", "No reasoning provided.")
+                                  print(f"    {i}. [{ '✅' if passed else '❌' }] {criterion}")
+                                  if not passed or args.debug:
+                                      print(f"       Reasoning: {reasoning}")
+                                      evidence = res.get("evidence", [])
+                                      if evidence:
+                                          print(f"       Evidence: {evidence}")
+                     print("\nValidation record saved.")
+            else:
+                 print(f"ERROR: Validation process failed for goal {args.goal_id}. Check logs for details.")
+            return success # Return the success status
+        except Exception as e:
+            print(f"CRITICAL ERROR during validation command for {args.goal_id}: {e}", file=sys.stderr)
+            logging.exception("Validation command failed") # Log full traceback
+            return False
+    # --------------------------------------------- 
+    
     # All other commands are synchronous, so just call them directly
     if args.command == "new":
         return create_new_goal(args.description)
@@ -3000,6 +3025,23 @@ simple remaining work, "mark_complete", "update_parent", or "give_up" in special
     
     args = parser.parse_args()
     
+    # Configure logging to ensure errors are visible
+    log_level = logging.INFO
+    # Check if args object has the attributes before accessing
+    if hasattr(args, 'debug') and args.debug:
+        log_level = logging.DEBUG
+    elif hasattr(args, 'quiet') and args.quiet:
+        log_level = logging.WARNING
+
+    # Ensure logs print to console
+    logging.basicConfig(
+        level=log_level,
+        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
+        handlers=[
+            logging.StreamHandler(sys.stdout) # Use stdout for INFO/DEBUG
+        ]
+    )
+
     # Run the main function
     main_command(args)
 
diff --git a/src/midpoint/utils/__init__.py b/src/midpoint/utils/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/src/midpoint/utils/config.py b/src/midpoint/utils/config.py
deleted file mode 100644
index a771fc6..0000000
--- a/src/midpoint/utils/config.py
+++ /dev/null
@@ -1,45 +0,0 @@
-"""
-Configuration utilities for Midpoint.
-"""
-import json
-import os
-from pathlib import Path
-from typing import Any, Dict, Optional
-
-def get_config_dir() -> Path:
-    """
-    Get the directory containing configuration files.
-    
-    Returns:
-        Path: Path to the configuration directory
-    """
-    # Check if we're running from the installed package or from the source directory
-    src_dir = Path(__file__).parent.parent
-    config_dir = src_dir / "config"
-    
-    if not config_dir.exists():
-        # Fallback to project root
-        config_dir = Path(__file__).parent.parent.parent.parent / "src" / "midpoint" / "config"
-    
-    return config_dir
-
-def load_config(config_name: str = "default.json") -> Dict[str, Any]:
-    """
-    Load a configuration file.
-    
-    Args:
-        config_name (str): Name of the config file to load. Defaults to "default.json".
-        
-    Returns:
-        Dict[str, Any]: Configuration data
-        
-    Raises:
-        FileNotFoundError: If the configuration file doesn't exist
-    """
-    config_path = get_config_dir() / config_name
-    
-    if not config_path.exists():
-        raise FileNotFoundError(f"Configuration file not found: {config_path}")
-    
-    with open(config_path, 'r') as f:
-        return json.load(f) 
\ No newline at end of file
diff --git a/src/midpoint/utils/logging.py b/src/midpoint/utils/logging.py
deleted file mode 100644
index c6984d7..0000000
--- a/src/midpoint/utils/logging.py
+++ /dev/null
@@ -1,112 +0,0 @@
-import os
-import time
-import uuid
-from typing import Optional
-from dataclasses import dataclass
-from datetime import datetime
-
-@dataclass
-class LogSession:
-    """Represents a logging session with unique ID and metadata."""
-    session_id: str
-    start_time: float
-    repository_path: str
-    initial_git_hash: str
-    goal_description: str
-
-class LogManager:
-    """Manages logging sessions and provides consistent log formatting."""
-    
-    def __init__(self, log_dir: str = "logs"):
-        """Initialize the log manager with a directory for log files."""
-        self.log_dir = log_dir
-        os.makedirs(log_dir, exist_ok=True)
-        self.current_session: Optional[LogSession] = None
-    
-    def start_session(self, repository_path: str, git_hash: str, goal_description: str) -> LogSession:
-        """Start a new logging session."""
-        session_id = str(uuid.uuid4())[:8]  # Use first 8 chars of UUID for readability
-        self.current_session = LogSession(
-            session_id=session_id,
-            start_time=time.time(),
-            repository_path=repository_path,
-            initial_git_hash=git_hash,
-            goal_description=goal_description
-        )
-        return self.current_session
-    
-    def get_session_log_path(self, log_type: str) -> str:
-        """Get the path for a session-specific log file."""
-        if not self.current_session:
-            raise RuntimeError("No active logging session")
-        return os.path.join(self.log_dir, f"{log_type}_{self.current_session.session_id}.log")
-    
-    def write_log_header(self, log_type: str, additional_info: dict = None):
-        """Write the header information to a log file."""
-        if not self.current_session:
-            raise RuntimeError("No active logging session")
-            
-        log_path = self.get_session_log_path(log_type)
-        with open(log_path, "w") as f:
-            f.write(f"Session ID: {self.current_session.session_id}\n")
-            f.write(f"Start Time: {datetime.fromtimestamp(self.current_session.start_time)}\n")
-            f.write(f"Repository: {self.current_session.repository_path}\n")
-            f.write(f"Initial Git Hash: {self.current_session.initial_git_hash}\n")
-            f.write(f"Goal: {self.current_session.goal_description}\n")
-            if additional_info:
-                for key, value in additional_info.items():
-                    f.write(f"{key}: {value}\n")
-            f.write("\n")
-    
-    def log_goal_decomposition(self, depth: int, parent_goal: str, subgoal: str, 
-                             branch_name: Optional[str] = None, git_hash: Optional[str] = None):
-        """Log a goal decomposition step with branch and git information."""
-        if not self.current_session:
-            raise RuntimeError("No active logging session")
-            
-        log_path = self.get_session_log_path("goal_hierarchy")
-        indent = "  " * depth
-        with open(log_path, "a") as f:
-            f.write(f"{indent}Goal: {parent_goal}\n")
-            f.write(f"{indent}└── Subgoal: {subgoal}\n")
-            if branch_name:
-                f.write(f"{indent}    Branch: {branch_name}\n")
-            if git_hash:
-                f.write(f"{indent}    Git Hash: {git_hash}\n")
-            f.flush()
-    
-    def log_execution_ready(self, depth: int, task: str, 
-                          branch_name: Optional[str] = None, git_hash: Optional[str] = None):
-        """Log when a task is ready for execution."""
-        if not self.current_session:
-            raise RuntimeError("No active logging session")
-            
-        log_path = self.get_session_log_path("goal_hierarchy")
-        indent = "  " * depth
-        with open(log_path, "a") as f:
-            f.write(f"{indent}✓ READY FOR EXECUTION: {task}\n")
-            if branch_name:
-                f.write(f"{indent}    Branch: {branch_name}\n")
-            if git_hash:
-                f.write(f"{indent}    Git Hash: {git_hash}\n")
-            f.flush()
-    
-    def log_execution_result(self, iteration: int, subgoal: str, git_hash: str, 
-                           branch_name: str, validation_score: float, execution_time: float):
-        """Log an execution result."""
-        if not self.current_session:
-            raise RuntimeError("No active logging session")
-            
-        log_path = self.get_session_log_path("execution")
-        with open(log_path, "a") as f:
-            f.write(f"\nExecution {iteration}:\n")
-            f.write(f"Subgoal: {subgoal}\n")
-            f.write(f"Branch: {branch_name}\n")
-            f.write(f"Git Hash: {git_hash}\n")
-            f.write(f"Validation Score: {validation_score:.2f}\n")
-            f.write(f"Execution Time: {execution_time:.2f}s\n")
-            f.write(f"Timestamp: {datetime.now()}\n")
-            f.flush()
-
-# Create a global log manager instance
-log_manager = LogManager() 
\ No newline at end of file
diff --git a/src/midpoint/validation.py b/src/midpoint/validation.py
index 7a2ae04..352b16b 100644
--- a/src/midpoint/validation.py
+++ b/src/midpoint/validation.py
@@ -10,13 +10,12 @@ import datetime
 import subprocess
 from pathlib import Path
 from typing import Dict, List, Any, Optional, Tuple
+import tempfile
+from .agents.tools.git_tools import get_repository_diff
+from .agents.tools.memory_tools import get_memory_diff
 
-# Import from midpoint modules
-from midpoint.goal_cli import (
-    ensure_goal_dir,
-    get_current_hash,
-    get_current_branch
-)
+# Import shared utilities
+from .utils import ensure_goal_dir, get_current_hash, get_current_branch
 
 # Import agents for automated validation
 from midpoint.agents.goal_validator import GoalValidator
@@ -217,71 +216,196 @@ async def perform_automated_validation(goal_id: str, goal_data: Dict[str, Any],
         dict: Dictionary with validation results containing:
             - criteria_results: List of dictionaries with validation results for each criterion
             - score: Overall validation score (0.0 to 1.0)
+            - timestamp: Timestamp of the validation attempt
     """
+    # --- Constants for diff size limits ---
+    CODE_DIFF_THRESHOLD = 5000
+    MEMORY_DIFF_THRESHOLD = 150000 # 30 * CODE_DIFF_THRESHOLD
+
     criteria = goal_data.get("validation_criteria", [])
-    
+    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S") # Generate timestamp early
+
     if not quiet:
         print("\nPerforming automated validation using LLM...")
-    
+
     if debug:
         print(f"Debug: Found {len(criteria)} validation criteria")
-    
+
     # Get the current repository path (assumed to be the current directory)
     repo_path = os.getcwd()
-    
-    # Get state information for constructing necessary objects
-    repo_state = await get_repository_state(goal_id, debug)
-    current_hash = repo_state["current_hash"]
-    branch_name = repo_state["current_branch"]
-    
-    # Create a Goal object from the goal data
-    from midpoint.agents.models import Goal, ExecutionResult, State
-    
+
+    # --- Get state information primarily from the goal file --- 
+    initial_state = goal_data.get("initial_state", {})
+    current_state = goal_data.get("current_state", {})
+
+    initial_git_hash = initial_state.get("git_hash")
+    # Use the git hash stored in the goal's current_state for comparison
+    current_hash = current_state.get("git_hash") 
+
+    # Get branch name - still useful, maybe from current_state or fallback to live state?
+    branch_name = current_state.get("branch_name") # Prefer branch from state
+    if not branch_name:
+         # Fallback: Get live branch name if not in state (less ideal for validating recorded state)
+         try:
+              from .utils import get_current_branch
+              branch_name = get_current_branch(repo_path) or "unknown"
+              logging.warning(f"Branch name not found in goal's current_state, using live branch: {branch_name}")
+         except Exception:
+              branch_name = "unknown"
+
+    if not current_hash:
+        logging.error(f"Final git_hash not found in goal's current_state for {goal_id}. Cannot perform validation.")
+        # Return failure result immediately
+        return {
+            "criteria_results": [{"criterion": c, "passed": False, "reasoning": "Validation failed: Missing final git_hash in goal file's current_state.", "evidence": []} for c in criteria],
+            "score": 0.0,
+            "timestamp": timestamp
+        }
+    # ----------------------------------------------------------
+
     # Get memory repository path and hash information
     memory_repo_path = None
-    memory_hash = None
     initial_memory_hash = None
-    
-    if "initial_state" in goal_data and "memory_repository_path" in goal_data["initial_state"]:
-        memory_repo_path = goal_data["initial_state"]["memory_repository_path"]
-    elif "memory" in goal_data and "repository_path" in goal_data["memory"]:
+    current_memory_hash = None
+
+    if "initial_state" in goal_data:
+        if "memory_repository_path" in goal_data["initial_state"]:
+            memory_repo_path = goal_data["initial_state"]["memory_repository_path"]
+        if "memory_hash" in goal_data["initial_state"]:
+            initial_memory_hash = goal_data["initial_state"]["memory_hash"]
+    # Fallbacks if not in initial_state
+    if not memory_repo_path and "memory" in goal_data and "repository_path" in goal_data["memory"]:
         memory_repo_path = goal_data["memory"]["repository_path"]
-    
+    if not initial_memory_hash and "memory" in goal_data and "initial_hash" in goal_data["memory"]:
+        initial_memory_hash = goal_data["memory"]["initial_hash"]
+
     if "current_state" in goal_data and "memory_hash" in goal_data["current_state"]:
-        memory_hash = goal_data["current_state"]["memory_hash"]
+        current_memory_hash = goal_data["current_state"]["memory_hash"]
+    # Fallback if not in current_state
     elif "memory" in goal_data and "hash" in goal_data["memory"]:
-        memory_hash = goal_data["memory"]["hash"]
-        
-    if "initial_state" in goal_data and "memory_hash" in goal_data["initial_state"]:
-        initial_memory_hash = goal_data["initial_state"]["memory_hash"]
-    elif "memory" in goal_data and "initial_hash" in goal_data["memory"]:
-        initial_memory_hash = goal_data["memory"]["initial_hash"]
-    
-    if debug and memory_repo_path:
-        print(f"Debug: Using memory repository path: {memory_repo_path}")
-    if debug and memory_hash:
-        print(f"Debug: Final memory hash: {memory_hash}")
-    if debug and initial_memory_hash:
-        print(f"Debug: Initial memory hash: {initial_memory_hash}")
-    
+        current_memory_hash = goal_data["memory"]["hash"]
+
+    if debug:
+        print(f"Debug: Initial Git Hash: {initial_git_hash}")
+        print(f"Debug: Final Git Hash: {current_hash}")
+        print(f"Debug: Initial Memory Hash: {initial_memory_hash}")
+        print(f"Debug: Final Memory Hash: {current_memory_hash}")
+        print(f"Debug: Code Repo Path: {repo_path}")
+        print(f"Debug: Memory Repo Path: {memory_repo_path}")
+
+    # --- Pre-calculate Diffs and Check Sizes ---
+    code_diff_content = None
+    memory_diff_content = None
+    code_diff_error = None
+    memory_diff_error = None
+
+    # Calculate Code Diff
+    if repo_path and initial_git_hash and current_hash:
+        if initial_git_hash != current_hash:
+            if not quiet: print("Calculating code diff...")
+            try:
+                code_diff_result = get_repository_diff(repo_path, initial_git_hash, current_hash)
+                code_diff_content = code_diff_result.get("diff_content", "")
+                # Check for error messages within the content if diff failed internally
+                if code_diff_result.get("error"):
+                     code_diff_error = code_diff_result.get("error")
+                     code_diff_content = None # Nullify content on error
+                elif code_diff_content is None: # Should not happen if no error, but check
+                     code_diff_error = "Unknown error retrieving code diff content."
+            except Exception as e:
+                code_diff_error = f"Failed to get code diff: {e}"
+                if debug: print(f"Debug: {code_diff_error}")
+        else:
+            code_diff_content = "(No changes: initial and final git hashes are identical)"
+    else:
+        code_diff_error = "Skipping code diff: Missing repo path or necessary hashes."
+        if debug: print(f"Debug: {code_diff_error}")
+
+    # Calculate Memory Diff
+    if memory_repo_path and initial_memory_hash and current_memory_hash:
+        if initial_memory_hash != current_memory_hash:
+            if not quiet: print("Calculating memory diff...")
+            try:
+                memory_diff_result = get_memory_diff(initial_memory_hash, current_memory_hash, memory_repo_path)
+                memory_diff_content = memory_diff_result.get("diff_content", "")
+                if memory_diff_result.get("error"):
+                    memory_diff_error = memory_diff_result.get("error")
+                    memory_diff_content = None # Nullify content on error
+                elif memory_diff_content is None:
+                     memory_diff_error = "Unknown error retrieving memory diff content."
+            except Exception as e:
+                memory_diff_error = f"Failed to get memory diff: {e}"
+                if debug: print(f"Debug: {memory_diff_error}")
+        else:
+             memory_diff_content = "(No changes: initial and final memory hashes are identical)"
+    elif memory_repo_path:
+        memory_diff_error = "Skipping memory diff: Missing initial or final memory hash."
+        if debug: print(f"Debug: {memory_diff_error}")
+    # else: No memory repo path configured, which is fine.
+
+    # Print sizes and check thresholds
+    if code_diff_content is not None:
+        code_diff_len = len(code_diff_content)
+        if not quiet: print(f"Code diff size: {code_diff_len} characters")
+        if code_diff_len > CODE_DIFF_THRESHOLD:
+            error_reason = f"Validation failed: Code diff size ({code_diff_len} chars) exceeds threshold ({CODE_DIFF_THRESHOLD} chars)."
+            logging.error(error_reason)
+            # Return failure result
+            return {
+                "criteria_results": [{"criterion": c, "passed": False, "reasoning": error_reason, "evidence": []} for c in criteria],
+                "score": 0.0,
+                "timestamp": timestamp
+            }
+    elif code_diff_error:
+         if not quiet: print(f"Code diff error: {code_diff_error}")
+         # Decide if diff error should cause validation failure. For now, let's proceed but LLM will need to handle it.
+         # If critical, uncomment below to fail:
+         # return {
+         #     "criteria_results": [{"criterion": c, "passed": False, "reasoning": f"Validation failed due to code diff error: {code_diff_error}", "evidence": []} for c in criteria],
+         #     "score": 0.0,
+         #     "timestamp": timestamp
+         # }
+
+    if memory_diff_content is not None:
+        memory_diff_len = len(memory_diff_content)
+        if not quiet: print(f"Memory diff size: {memory_diff_len} characters")
+        if memory_diff_len > MEMORY_DIFF_THRESHOLD:
+            error_reason = f"Validation failed: Memory diff size ({memory_diff_len} chars) exceeds threshold ({MEMORY_DIFF_THRESHOLD} chars)."
+            logging.error(error_reason)
+            # Return failure result
+            return {
+                "criteria_results": [{"criterion": c, "passed": False, "reasoning": error_reason, "evidence": []} for c in criteria],
+                "score": 0.0,
+                "timestamp": timestamp
+            }
+    elif memory_diff_error:
+         if not quiet: print(f"Memory diff error: {memory_diff_error}")
+         # Let LLM handle memory diff errors unless critical.
+
+    # --- Prepare Goal Object and Validator ---
+    from midpoint.agents.models import Goal, ExecutionResult, State
+
     goal = Goal(
         description=goal_data.get("description", ""),
         validation_criteria=criteria,
         success_threshold=goal_data.get("success_threshold", 0.8),
         initial_state=State(
-            git_hash=goal_data.get("initial_state", {}).get("git_hash"),
+            git_hash=initial_git_hash,
             memory_hash=initial_memory_hash,
             repository_path=repo_path,
-            memory_repository_path=memory_repo_path
+            memory_repository_path=memory_repo_path,
+            description="Initial state for validation"
         ),
         current_state=State(
             git_hash=current_hash,
-            memory_hash=memory_hash,
+            memory_hash=current_memory_hash,
             repository_path=repo_path,
-            memory_repository_path=memory_repo_path
+            memory_repository_path=memory_repo_path,
+            description="Current state for validation"
         )
+        # Metadata will be added below inside the try block
     )
-    
+
     # Create a mock ExecutionResult to use with the goal validator
     execution_result = ExecutionResult(
         success=True,
@@ -290,66 +414,111 @@ async def perform_automated_validation(goal_id: str, goal_data: Dict[str, Any],
         repository_path=repo_path,
         memory_repository_path=memory_repo_path
     )
-    
+
     # Initialize the GoalValidator
     validator = GoalValidator()
-    
+
+    # --- Execute Validation with Temp Files ---
+    code_diff_file = None
+    memory_diff_file = None
+    code_diff_file_path = None
+    memory_diff_file_path = None
+    validation_result = None
+
     try:
-        # Validate the goal execution
+        # Create temp file for code diff if content exists
+        if code_diff_content is not None:
+            code_diff_file = tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix=".diff", prefix=f"goal_{goal_id}_code_")
+            code_diff_file.write(code_diff_content)
+            code_diff_file_path = code_diff_file.name
+            code_diff_file.close() # Close the file handle but don't delete yet
+            if debug: print(f"Debug: Created temp code diff file: {code_diff_file_path}")
+
+        # Create temp file for memory diff if content exists
+        if memory_diff_content is not None:
+            memory_diff_file = tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix=".diff", prefix=f"goal_{goal_id}_memory_")
+            memory_diff_file.write(memory_diff_content)
+            memory_diff_file_path = memory_diff_file.name
+            memory_diff_file.close() # Close the file handle but don't delete yet
+            if debug: print(f"Debug: Created temp memory diff file: {memory_diff_file_path}")
+
+        # Add paths and errors to goal metadata for the validator agent
+        goal.metadata = {
+            "code_diff_path": code_diff_file_path,
+            "memory_diff_path": memory_diff_file_path,
+            "code_diff_error": code_diff_error,
+            "memory_diff_error": memory_diff_error
+        }
+
+        # Validate the goal execution using the modified goal object
         if not quiet:
-            print(f"Validating goal: {goal_data.get('description', '')}")
+            print(f"Validating goal: {goal.description}")
             print(f"Validation criteria: {len(criteria)}")
-        
-        validation_result = await validator.validate_execution(goal, execution_result)
-        
-        # Generate timestamp for file naming
-        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
-        
+
+        validation_result_obj = validator.validate_execution(goal, execution_result)
+
         # Save the validation context (including system prompt and other prompts)
         if hasattr(validator, 'last_validation_messages') and validator.last_validation_messages:
             await save_validation_context(goal_id, timestamp, validator.last_validation_messages)
             if not quiet:
                 print("Saved validation context to file")
-        
+
         # Convert the validation result to criteria_results format
         criteria_results = []
-        for result in validation_result.criteria_results:
+        for result in validation_result_obj.criteria_results:
             criteria_results.append({
                 "criterion": result.criterion if hasattr(result, 'criterion') else "",
                 "passed": result.passed if hasattr(result, 'passed') else False,
                 "reasoning": result.reasoning if hasattr(result, 'reasoning') else "",
                 "evidence": result.evidence if hasattr(result, 'evidence') else []
             })
-        
+
         if debug:
-            print(f"Debug: Validation complete with score {validation_result.score:.2f}")
-            print(f"Debug: Reasoning: {validation_result.reasoning}")
-        
-        return {
+            print(f"Debug: Validation complete with score {validation_result_obj.score:.2f}")
+            print(f"Debug: Reasoning: {validation_result_obj.reasoning}")
+
+        validation_result = {
             "criteria_results": criteria_results,
-            "score": validation_result.score,
+            "score": validation_result_obj.score,
             "timestamp": timestamp
         }
+
     except Exception as e:
         if debug:
-            print(f"Debug: Error during LLM validation: {str(e)}")
-        logging.error(f"Error during automated validation: {str(e)}")
-        
+            print(f"Debug: Error during LLM validation execution: {str(e)}")
+        logging.error(f"Error during automated validation execution: {str(e)}")
+
         # Return failed validation results
         criteria_results = []
         for criterion in criteria:
             criteria_results.append({
                 "criterion": criterion,
                 "passed": False,
-                "reasoning": f"Validation failed due to error: {str(e)}",
+                "reasoning": f"Validation failed due to system error during execution: {str(e)}",
                 "evidence": []
             })
-        
-        return {
+
+        validation_result = {
             "criteria_results": criteria_results,
             "score": 0.0,
-            "timestamp": datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
+            "timestamp": timestamp
         }
+    finally:
+        # --- Cleanup Temp Files ---
+        if code_diff_file_path and os.path.exists(code_diff_file_path):
+            try:
+                os.remove(code_diff_file_path)
+                if debug: print(f"Debug: Removed temp code diff file: {code_diff_file_path}")
+            except OSError as e:
+                logging.warning(f"Could not remove temp file {code_diff_file_path}: {e}")
+        if memory_diff_file_path and os.path.exists(memory_diff_file_path):
+             try:
+                 os.remove(memory_diff_file_path)
+                 if debug: print(f"Debug: Removed temp memory diff file: {memory_diff_file_path}")
+             except OSError as e:
+                 logging.warning(f"Could not remove temp file {memory_diff_file_path}: {e}")
+
+    return validation_result
 
 
 async def validate_goal(goal_id: str, debug: bool = False, quiet: bool = False, auto: bool = False) -> Tuple[bool, Dict[str, Any]]:
